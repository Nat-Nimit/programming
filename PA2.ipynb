{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nat-Nimit/programming/blob/master/PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G-2dSGWEtXB-",
        "colab_type": "code",
        "outputId": "bcf81c1f-ad1d-42ca-94a9-5630bc866960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XhjW84Ndtc-Z",
        "colab_type": "code",
        "outputId": "7cf7d014-ca48-464c-a6c1-60ee27189504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "cd drive/'My Drive'/PA/nlp/nlp"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/PA/nlp/nlp'\n",
            "/content/drive/My Drive/PA/nlp/nlp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NrTUq8HYuVIP",
        "colab_type": "code",
        "outputId": "0873c763-94ad-4515-95c5-345fd1bb8f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/e7/6d31a11de701d820df630e3fd0e4038efbd86aa73f76198539c7d59652ea/pythainlp-1.7.3-py3-none-any.whl (10.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.3MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pythainlp) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pythainlp) (1.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pythainlp) (4.28.1)\n",
            "Collecting marisa-trie (from pythainlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K    100% |████████████████████████████████| 276kB 24.2MB/s \n",
            "\u001b[?25hCollecting tinydb (from pythainlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/2b/98040184cfbf03113736a160ea35aa92dc3619312ba5a4d6cafaf7f81c73/tinydb-3.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk>=3.2.2 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (3.2.5)\n",
            "Collecting conllu (from pythainlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ca/82/b02495f1c594cfb4af9b1eb8f404e35c1298a1448fc950b37f14c3e83317/conllu-1.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2.18.4)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (0.16.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2018.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (1.22)\n",
            "Building wheels for collected packages: marisa-trie\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "Successfully built marisa-trie\n",
            "Installing collected packages: marisa-trie, tinydb, conllu, pythainlp\n",
            "Successfully installed conllu-1.2.3 marisa-trie-0.7.5 pythainlp-1.7.3 tinydb-3.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ORjtKVDudqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "data = open(\"title_classification.train\")\n",
        "title = []\n",
        "label = []\n",
        "for line in data.readlines():\n",
        "  data_list = line.strip(\"\\n\").split(\"\\t\",1)\n",
        "  label.append(data_list[0])\n",
        "  title.append(data_list[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Vt5Hl88Imj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feature_matrices(token_list):\n",
        "  feature_matrix = []\n",
        "  for i in range(0,len(token_list)):\n",
        "    vocab = {token:1 for token in token_list[i]}\n",
        "    feature_matrix.append(vocab)\n",
        "  return feature_matrix\n",
        "\n",
        "def clean_data(token_list):\n",
        "  clean_list = []\n",
        "  erase_list = [\"\\'\",\"\\\"\", \",\",\":\"]\n",
        "  for i in range(0,len(token_list)):\n",
        "    cleaned = \" \"\n",
        "    for alpha in token_list:\n",
        "      if alpha in erase_list:\n",
        "        cleaned = cleaned + token_list[i].strip(alpha)\n",
        "        clean_list.append(cleaned)\n",
        "  return clean_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQDPzU1Yu3e0",
        "colab_type": "code",
        "outputId": "c452ccba-cbdd-4da4-f08d-ed113f6d90d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "feature_matrix_train = feature_matrices(title)\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "dv = DictVectorizer(sparse=True)\n",
        "sparse_feature_matrix_train = dv.fit_transform(feature_matrix_train)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(sparse_feature_matrix_train, label)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "6DNIhbTzKjyz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "import joblib\n",
        "class MaxEntTopicClassifier:\n",
        "\n",
        "    def load_model(self, model_file_name:str):\n",
        "      mapping, parameter = joblib.load(model_file_name)\n",
        "      self.modelmaxent = parameter\n",
        "      self.mapping = mapping\n",
        "\n",
        "    def classify(self, title_text:str) -> str: \n",
        "      text = {token : 1 for token in word_tokenize(title_text)}\n",
        "      answer = self.modelmaxent.predict(self.mapping.transform(text))\n",
        "      return answer\n",
        "\n",
        "    def classify_list(self, title_text_list:list) -> list:\n",
        "        \"\"\"Classify a list of titles\n",
        "\n",
        "        Some models are faster to classify a list of titles than for-loop over each one.\n",
        "        You should override this method. \n",
        "        \"\"\"\n",
        "        answer_list = [self.classify(title) for title in title_text_list ]\n",
        "        return answer_list\n",
        "\n",
        "      \n",
        "import random\n",
        "class RandomTopicClassifier:\n",
        "\n",
        "    def load_model(self, model_file_name:str):\n",
        "        \"\"\"Load the parameters (model) from files\n",
        "\n",
        "        The parameters for this classifier do not do anything useful.\n",
        "        This is just an example\n",
        "        \"\"\"\n",
        "        mapping, parameter = joblib.load(model_file_name)\n",
        "        self.mapping = mapping\n",
        "        self.parameter = parameter\n",
        "        random.seed(parameter)\n",
        "        \n",
        "\n",
        "    def classify(self, title_text:str) -> str:\n",
        "        \"\"\"Classify randomly\n",
        "\n",
        "        This classifier returns a random label string\n",
        "        \"\"\"\n",
        "        num_classes = len(self.parameter.classes_)\n",
        "        classification = random.randint(0, num_classes - 1)\n",
        "        return self.parameter.classes_[classification]\n",
        "    \n",
        "    def classify_list(self, title_list_names):\n",
        "      answer = [self.classify(title) for title in title_list_names]\n",
        "      return answer\n",
        "\n",
        "class DANTopicClassifier:\n",
        "\n",
        "    def load_model(self, model_file_name:str):\n",
        "        pass\n",
        "\n",
        "    def classify(self, title_text:str) -> str: \n",
        "        return 'การเมือง'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zN_nDXuz2Qi4",
        "colab_type": "code",
        "outputId": "d0c10952-01d2-4c8b-95f0-f7d7c0818532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Run all of the classifiers on training and dev data\n",
        "\n",
        "To make sure that their accuracies look OK.\n",
        "You must not change anything in this file.\n",
        "\"\"\"\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from topic_classifier import TopicClassifier, MajorityTopicClassifier\n",
        "#import RandomTopicClassifier\n",
        "#from maxent_topic_classifier  import MaxEntTopicClassifier\n",
        "from dan_topic_classifier import DANTopicClassifier\n",
        "from dan_topic_classifier2 import DANTopicClassifier2\n",
        "\n",
        "def read_data(data_file_name:str):\n",
        "    labels = []\n",
        "    titles = []\n",
        "    with open(data_file_name) as f:\n",
        "        for line in f:\n",
        "            label, title = line.split('\\t', 1)\n",
        "            labels.append(label)\n",
        "            titles.append(title)\n",
        "    return (labels, titles)       \n",
        "\n",
        "def evaluate_classifier(classifier:TopicClassifier, data_file_name:str):\n",
        "    true_labels, titles = read_data(data_file_name)\n",
        "    prediction = classifier.classify_list(titles)\n",
        "    #print (classification_report(true_labels, prediction))\n",
        "    print (accuracy_score(true_labels, prediction))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    rtc = RandomTopicClassifier()\n",
        "    rtc.load_model('model.bin')\n",
        "    print(rtc.classify('นายกลุยภาคใต้'))\n",
        "    print('Random classification train and test accuracies')\n",
        "    evaluate_classifier(rtc, 'title_classification.train')\n",
        "    evaluate_classifier(rtc, 'title_classification.dev')\n",
        "\n",
        "    '''mjc = MajorityTopicClassifier()\n",
        "    print('Majority classification train and test accuracies')\n",
        "    evaluate_classifier(mjc, 'title_classification.train')\n",
        "    evaluate_classifier(mjc, 'title_classification.dev')'''\n",
        " \n",
        "    maxent = MaxEntTopicClassifier()\n",
        "    maxent.load_model('model.bin')\n",
        "    print('MaxEnt model train and test accuracies')\n",
        "    evaluate_classifier(maxent, 'title_classification.train')\n",
        "    evaluate_classifier(maxent, 'title_classification.dev')\n",
        "\n",
        "    '''dan1 = DANTopicClassifier('dan_model.bin')\n",
        "    print('DAN model train and test accuracies')\n",
        "    evaluate_classifier(dan1, 'title_classification.train')\n",
        "    evaluate_classifier(dan1, 'title_classification.dev')\n",
        "\n",
        "    dan2 = DANTopicClassifier2('dan2_model.bin')\n",
        "    print ('DAN model2 train and test accuracies')\n",
        "    evaluate_classifier(dan2, 'title_classification.train')\n",
        "    evaluate_classifier(dan2, 'title_classification.dev')'''"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "สิ่งแวดล้อม\n",
            "Random classification train and test accuracies\n",
            "0.08268124407609055\n",
            "0.08656071593889832\n",
            "MaxEnt model train and test accuracies\n",
            "0.8585535741838781\n",
            "0.6315383428483259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "APkTM-ocVMwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "3caac0bf-20ea-4914-eb8d-24510ae75a96"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.arts.chula.ac.th/ling/wp-content/uploads/TNCc5model.bin  "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-02 02:57:25--  http://www.arts.chula.ac.th/ling/wp-content/uploads/TNCc5model.bin\n",
            "Resolving www.arts.chula.ac.th (www.arts.chula.ac.th)... 161.200.48.9\n",
            "Connecting to www.arts.chula.ac.th (www.arts.chula.ac.th)|161.200.48.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39383847 (38M) [application/octet-stream]\n",
            "Saving to: ‘TNCc5model.bin.1’\n",
            "\n",
            "TNCc5model.bin.1    100%[===================>]  37.56M  7.25MB/s    in 5.4s    \n",
            "\n",
            "2019-03-02 02:57:31 (6.91 MB/s) - ‘TNCc5model.bin.1’ saved [39383847/39383847]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tdFdwr91Vb1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "9b18eccc-08b5-4a89-ad30-f34a210d2b76"
      },
      "cell_type": "code",
      "source": [
        "!pip install tltk"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/a8/89c69547602f894f87df586ec42033b94ad2e55df7771a0096fa49487926/tltk-1.2.1.tar.gz (10.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.5MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from tltk) (3.2.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from tltk) (0.0)\n",
            "Collecting sklearn_crfsuite (from tltk)\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from tltk) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->tltk) (1.11.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->tltk) (0.20.2)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite->tltk) (4.28.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite->tltk) (0.8.3)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite->tltk)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K    100% |████████████████████████████████| 757kB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim->tltk) (1.14.6)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->tltk) (1.8.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->tltk) (1.1.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->tltk) (2.49.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->tltk) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->tltk) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->tltk) (1.9.103)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->tltk) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->tltk) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->tltk) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->tltk) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->tltk) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.103 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->tltk) (1.12.103)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->tltk) (0.2.0)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.103->boto3->smart-open>=1.2.1->gensim->tltk) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.103->boto3->smart-open>=1.2.1->gensim->tltk) (2.5.3)\n",
            "Building wheels for collected packages: tltk\n",
            "  Building wheel for tltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/12/7b/32/5dccd202a0896e39b2adf82252e7e508560ce64e0a6bdbec6a\n",
            "Successfully built tltk\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, tltk\n",
            "Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6 tltk-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "raQT-WhGZKzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "96d60ec1-6c44-4b2c-b29e-3bc9980b1b1c"
      },
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "import tltk\n",
        "def clean_data(title):\n",
        "  tk = word_tokenize(title)\n",
        "  erase_list = [\"\\'\",\"\\\"\", \",\",\":\",\" \",\"\\s\", \"-\"]\n",
        "  for i in range(0, len(tk)):\n",
        "    if tk[i] != \"\":\n",
        "      for e in erase_list:\n",
        "         y = tk[i].strip(e)\n",
        "         tk[i] = y\n",
        "  return tk\n",
        "\n",
        "tt = clean_data(title[10])\n",
        "\n",
        "def title_to_w2v(title_tokens):\n",
        "  tltk.corpus.w2v_load()\n",
        "  vect = [tltk.corpus.w2v(t)for t in tk]\n",
        "  new_vect = [ v  for v in vect if v != ()]\n",
        "  total = len(new_vect)\n",
        "  score = sum(new_vect)/total\n",
        "  #print(title_tokens, len(vect), total)\n",
        "  return score\n",
        "\n",
        "title_to_w2v(tt)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.66975874, -0.07039134, -0.6568924 , -0.35951537,  0.807786  ,\n",
              "       -0.78304064, -0.363347  ,  0.06952314, -0.5563734 ,  0.3152643 ,\n",
              "       -0.32270417,  0.3132618 ,  0.14647278, -0.0722758 ,  0.1897187 ,\n",
              "       -0.63453496, -1.0876755 ,  0.55601585, -1.5317719 ,  0.73401904,\n",
              "       -0.20480767, -0.27491006,  1.1255376 , -0.61270654, -0.8677467 ,\n",
              "       -1.1733923 ,  0.73311245,  0.24002978, -0.5794016 ,  0.95699805,\n",
              "        0.5726679 ,  0.12154492, -0.01697617, -0.14300957, -0.1332423 ,\n",
              "        0.45896286, -0.40979868, -0.49533957,  0.27328616, -0.70939416,\n",
              "        0.11675704, -0.32048202,  0.01758637,  0.7025831 , -0.12782298,\n",
              "       -0.43072253,  0.21401462, -0.60836124,  0.30287063,  0.33807027,\n",
              "       -0.4808868 ,  0.4368609 ,  0.16393843,  0.22849007, -0.19197884,\n",
              "        0.33541617,  0.3229007 ,  0.9921147 ,  0.11113813, -1.0551497 ,\n",
              "        0.2308183 ,  0.23236394, -1.1600754 , -0.5617396 ,  0.59898245,\n",
              "        0.34760067, -1.3854996 ,  0.9512762 , -0.9568508 , -0.42112145,\n",
              "        0.146109  ,  0.07487081, -0.6865675 ,  0.607741  ,  0.9414381 ,\n",
              "       -0.4700868 , -0.28393316, -0.07338862,  0.5308764 ,  0.10904589,\n",
              "       -0.17319587,  1.2070695 ,  0.47152632, -0.38007894, -0.12044907,\n",
              "        0.0922157 ,  0.97350943, -0.32067788,  0.4994344 ,  0.18949577,\n",
              "       -0.40897378,  0.27740794,  0.20180765,  0.3633191 , -0.22768421,\n",
              "       -0.53978854,  0.9585695 , -0.5509991 ,  0.15756698,  0.10729229],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "eTxbiWEFtUK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embedding_size = 100\n",
        "num_data = len(title)\n",
        "feature_matrix = np.zeros((num_data, embedding_size))\n",
        "for num in range(0,num_data):\n",
        "  tokens = clean_data(title[num])\n",
        "  feature_matrix[num] = title_to_w2v(tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}